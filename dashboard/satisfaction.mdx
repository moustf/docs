---
title: 'Customer Satisfaction & Outcomes'
description: 'Track customer satisfaction metrics including CSAT scores, NPS, sentiment analysis, first-call resolution rates, and escalation patterns to optimize your voice AI experience'
---

# Customer Satisfaction & Outcomes

The Satisfaction & Outcome tab provides comprehensive insights into how well your voice AI system meets customer needs. Track satisfaction scores, analyze sentiment, measure resolution effectiveness, and identify opportunities to improve the customer experience.

## Overview

Technical performance matters, but customer satisfaction is the ultimate measure of success. A fast system that doesn't solve problems is worthless. The Satisfaction tab helps you:

- **Measure satisfaction**: Track CSAT, NPS, and sentiment metrics
- **Understand outcomes**: Monitor resolution rates and escalation patterns
- **Identify issues**: Spot trends in negative sentiment or high escalations
- **Optimize experience**: Use data to improve agent training and workflows
- **Prove ROI**: Demonstrate the business value of your voice AI investment

<Info>
Satisfaction metrics provide the "why" behind your performance data. High latency may correlate with low satisfaction, or great satisfaction may justify slightly slower but more thorough responses.
</Info>

## Accessing Satisfaction Metrics

1. Navigate to **Dashboard** from the main menu
2. Click the **Satisfaction & Outcome** tab
3. Apply filters to analyze specific segments:
   - **Agent Filter**: Compare satisfaction across agents
   - **Date Range**: Track satisfaction trends over time

```
Dashboard > Satisfaction & Outcome tab
```

The tab automatically refreshes when you return to it from other views or after closing call details.

## Key Satisfaction Metrics

The Satisfaction tab displays five primary metrics that measure different aspects of customer experience:

### CSAT Score

**What it measures**: Customer Satisfaction Score - the average rating customers give after calls

**Displayed as**: Percentage (%)

**How it's calculated**:

```
CSAT = (Number of Satisfied Customers / Total Survey Responses) Ã— 100

Typically:
- Ratings 4-5 out of 5 = Satisfied
- Ratings 1-3 out of 5 = Not Satisfied

Example:
80 customers rated 4 or 5 stars
100 total survey responses
CSAT = (80 / 100) Ã— 100 = 80%
```

**Benchmarks**:

```
Excellent: > 85%
Good: 75% - 85%
Acceptable: 65% - 75%
Needs Improvement: < 65%
```

**Why it matters**:

- **Direct feedback**: Customers tell you if they're satisfied
- **Simple to understand**: Easy metric to communicate to stakeholders
- **Actionable**: Low CSAT points to specific problems
- **Correlates with retention**: High CSAT = customers return

<Tip>
CSAT is most meaningful when you get high survey response rates. If only 5% of customers respond, results may be biased (typically only very happy or very unhappy respond). Aim for at least 20-30% response rate.
</Tip>

**Common CSAT patterns and what they mean**:

```
Pattern 1: High CSAT (>85%)
Meaning: System is meeting customer needs well
Actions:
- Maintain current approach
- Document what's working
- Look for incremental improvements
- Use as benchmark for new agents

Pattern 2: Moderate CSAT (65-75%)
Meaning: System works but has room for improvement
Actions:
- Review negative feedback for themes
- Compare to escalation rate (are unresolved issues the problem?)
- Test agent improvements
- Consider user experience enhancements

Pattern 3: Low CSAT (<65%)
Meaning: Significant issues affecting satisfaction
Urgent actions:
- Review recent calls for common problems
- Check if technical issues (high latency, errors)
- Analyze sentiment for specific pain points
- Consider agent training overhaul

Pattern 4: Declining CSAT (trending down)
Meaning: Something changed for the worse
Investigation:
- Did we make a recent change?
- Is competition improving?
- Are expectations changing?
- Review customer comments for clues
```

<Warning>
A sudden drop in CSAT should trigger immediate investigation. Check for recent deployments, configuration changes, or external factors (competitor launch, seasonal issues, etc.).
</Warning>

### NPS (Net Promoter Score)

**What it measures**: Likelihood that customers would recommend your service to others

**Displayed as**: Score from -100 to +100

**How it's calculated**:

```
Customers rate 0-10: "How likely are you to recommend us?"

Promoters (9-10): Enthusiastic supporters
Passives (7-8): Satisfied but unenthusiastic
Detractors (0-6): Unhappy, may damage brand

NPS = % Promoters - % Detractors

Example:
40% Promoters
10% Detractors
NPS = 40 - 10 = 30
```

**Benchmarks**:

```
World Class: > 70
Excellent: 50 - 70
Good: 30 - 50
Acceptable: 0 - 30
Needs Work: < 0 (more detractors than promoters)
```

**Why it matters**:

- **Growth indicator**: High NPS drives word-of-mouth growth
- **Loyalty measure**: Predicts customer retention
- **Competitive benchmark**: Compare to industry standards
- **Investment justification**: Proves customer advocacy

**NPS interpretation**:

<Tabs>
  <Tab title="Promoters (9-10)">
    **Characteristics**:
    - Will recommend to friends
    - Likely to return
    - Forgive minor issues
    - Provide positive reviews

    **Actions**:
    - Ask for testimonials/reviews
    - Create case studies
    - Understand what delighted them
    - Replicate success with other customers

  </Tab>

  <Tab title="Passives (7-8)">
    **Characteristics**:
    - Satisfied but not enthusiastic
    - Vulnerable to competition
    - Won't actively promote
    - Neutral reviews if any

    **Actions**:
    - Find ways to delight them
    - Address their specific concerns
    - Turn them into promoters
    - Prevent defection to competitors

  </Tab>

  <Tab title="Detractors (0-6)">
    **Characteristics**:
    - Unhappy with experience
    - May leave negative reviews
    - Won't recommend
    - At risk of churning

    **Actions**:
    - Immediate follow-up
    - Understand root cause
    - Fix systemic issues
    - Attempt to recover relationship

  </Tab>
</Tabs>

**Using NPS for improvement**:

```
Strategy 1: Close the loop
- Follow up with detractors personally
- Ask: "What would have made this a 9 or 10?"
- Use feedback to prioritize fixes

Strategy 2: Identify patterns
- Group detractors by common issues
- "Voice AI was too slow" (50% of detractors)
- "Couldn't solve my problem" (30% of detractors)
- Tackle the biggest issues first

Strategy 3: Learn from promoters
- What made them rate 9-10?
- Can we replicate those conditions?
- Use promoter quotes in marketing
```

<Note>
NPS varies significantly by industry. Research typical NPS scores for your sector to set realistic targets. B2B SaaS average is around 30, while e-commerce might be 40-50.
</Note>

### First-Call Resolution (FCR)

**What it measures**: Percentage of calls resolved without need for escalation, callback, or follow-up

**Displayed as**: Percentage (%)

**How it's calculated**:

```
FCR = (Calls Resolved on First Contact / Total Calls) Ã— 100

Resolved = Customer's issue addressed completely
Not Resolved = Escalated, transferred, or requires follow-up

Example:
850 calls resolved on first contact
1,000 total calls
FCR = (850 / 1,000) Ã— 100 = 85%
```

**Benchmarks**:

```
Excellent: > 80%
Good: 70% - 80%
Acceptable: 60% - 70%
Needs Improvement: < 60%
```

**Why it matters**:

- **Efficiency**: High FCR means fewer repeat contacts
- **Cost savings**: Each repeat contact costs money
- **Satisfaction**: Customers prefer one-and-done resolution
- **AI effectiveness**: Measures how well AI handles queries

**What "resolved" means**:

```
Resolved:
âœ“ Customer's question answered completely
âœ“ Issue fixed without escalation
âœ“ Customer confirmed satisfaction
âœ“ No follow-up needed

Not Resolved:
âœ— Escalated to human agent
âœ— Customer needed to call back
âœ— Issue partially addressed
âœ— Customer disconnected frustrated
```

<CardGroup cols={2}>
  <Card title="High FCR = Happy Customers" icon="smile">
    First-call resolution is one of the strongest predictors of customer satisfaction. Solve it once, solve it right.
  </Card>
  <Card title="Low FCR = Hidden Costs" icon="money-bill">
    Every repeat contact doubles your cost to serve. FCR improvement directly impacts the bottom line.
  </Card>
</CardGroup>

**Improving FCR**:

```
Tactic 1: Identify common repeat issues
- Review escalated calls
- What could AI have handled?
- Train agent on those scenarios

Tactic 2: Empower the AI
- Give access to more data/tools
- Expand what AI can do autonomously
- Reduce handoff friction

Tactic 3: Set clear expectations
- Tell customer what AI can/can't do upfront
- Offer human option early if complex
- Better to escalate early than frustrate customer

Tactic 4: Verify understanding
- Ask customer: "Did this fully answer your question?"
- Offer "anything else I can help with?"
- Confirm before ending call

Tactic 5: Analyze partial resolutions
- Where does AI get stuck?
- Missing data? Limited authority?
- Fix root causes
```

**FCR vs. other metrics**:

```
Relationship with Escalation Rate:
- High FCR = Low Escalation Rate (inverse relationship)
- Target: FCR > 75%, Escalation < 25%

Relationship with CSAT:
- High FCR usually = High CSAT
- If FCR is high but CSAT is low:
  â†’ AI might be "resolving" incorrectly
  â†’ Review if resolutions are actually correct

Relationship with Duration:
- Very high FCR + Very short duration:
  â†’ May be too quick, not thorough
- High FCR + Reasonable duration:
  â†’ Good balance of speed and completeness
```

<Warning>
Don't artificially inflate FCR by marking calls as "resolved" when they're not. This creates a false sense of success while customer satisfaction plummets.
</Warning>

### Escalation Rate

**What it measures**: Percentage of calls that required transfer to a human agent

**Displayed as**: Percentage (%)

**How it's calculated**:

```
Escalation Rate = (Escalated Calls / Total Calls) Ã— 100

Escalated = Transferred to human agent
Not Escalated = AI handled completely

Example:
150 calls escalated to humans
1,000 total calls
Escalation Rate = (150 / 1,000) Ã— 100 = 15%
```

**Benchmarks**:

```
Excellent: < 10%
Good: 10% - 20%
Acceptable: 20% - 30%
High: > 30%
```

**Why it matters**:

- **AI containment**: Lower rate = AI handles more independently
- **Cost efficiency**: Human agents are expensive
- **ROI measurement**: Proves AI is reducing human workload
- **Complexity indicator**: High rate may mean queries are too complex for AI

**Understanding escalation patterns**:

<Tabs>
  <Tab title="Low Escalation (<10%)">
    **What it means**:
    - AI handles most queries successfully
    - Well-trained agent with broad knowledge
    - Users have appropriate expectations

    **Considerations**:
    - Verify CSAT is still high (not just avoiding escalation)
    - Check if AI is forcing solutions when escalation would be better
    - Ensure customers know human option exists

    **Example use cases**:
    - Simple FAQ bots
    - Order status lookups
    - Account balance inquiries

  </Tab>

  <Tab title="Moderate Escalation (10-20%)">
    **What it means**:
    - Balanced AI/human approach
    - AI handles common issues, escalates complex ones
    - Appropriate for most customer service use cases

    **Optimization**:
    - Review escalated calls for training opportunities
    - Identify top 3 escalation reasons
    - Train AI on those scenarios

    **Example use cases**:
    - Customer support helplines
    - Technical support (Tier 1)
    - Billing inquiries

  </Tab>

  <Tab title="High Escalation (>30%)">
    **What it means**:
    - AI is not handling enough autonomously
    - Queries may be too complex
    - AI may be undertrained

    **Urgent actions**:
    - Analyze why escalations are happening
    - Expand AI's capabilities
    - Improve agent training
    - Consider if use case is appropriate for voice AI

    **Possible causes**:
    - Complex domain (medical, legal)
    - Insufficient training data
    - Limited system integration
    - User skepticism of AI

  </Tab>
</Tabs>

**Escalation reasons to track**:

```
Technical Reasons:
- AI doesn't understand query
- AI lacks necessary data/access
- System error or timeout
- Language/accent not supported

Business Reasons:
- Query outside AI's scope
- Requires human judgment
- High-value customer request
- Compliance/legal requirement

User Preference:
- Customer explicitly requests human
- Frustrated with AI
- Complex emotional situation
- Privacy/security concerns
```

**Optimal escalation strategy**:

```
Early escalation indicators:
1. Multiple clarification attempts
   - User doesn't understand AI
   - AI doesn't understand user
   â†’ Escalate after 3 failed attempts

2. Emotional distress detected
   - Anger, frustration in voice/words
   â†’ Immediate escalation to defuse

3. High-value scenario
   - Large order, important account
   â†’ Route to human for white-glove service

4. Explicit request
   - "I want to talk to a person"
   â†’ Escalate immediately, don't argue

Smart escalation approach:
- Set clear expectations: "I'll connect you with a specialist"
- Pass context: Give human agent conversation history
- Track outcome: Did human resolve it? Could AI learn?
```

<Tip>
The goal isn't zero escalationsâ€”it's optimal escalations. Some queries should go to humans. Focus on reducing unnecessary escalations while ensuring smooth handoffs when needed.
</Tip>

### Sentiment Distribution

**What it measures**: The emotional tone of customer conversations (Positive, Neutral, Negative)

**Displayed as**: Percentage breakdown and donut chart

**How it's calculated**:

```
AI analyzes conversation transcripts for sentiment:
- Positive (0.6 - 1.0): Happy, satisfied, grateful
- Neutral (0.4 - 0.6): Matter-of-fact, transactional
- Negative (0.0 - 0.4): Frustrated, angry, upset

Percentages:
- Positive: 65%
- Neutral: 25%
- Negative: 10%
(Total = 100%)
```

**Benchmarks**:

```
Healthy distribution:
- Positive: 60-70%
- Neutral: 20-30%
- Negative: 5-15%

Warning signs:
- Negative > 20% â†’ Serious issues
- Positive < 50% â†’ Not delighting customers
- Neutral > 50% â†’ Transactional, not engaging
```

**Why it matters**:

- **Early warning system**: Catch issues before they affect CSAT
- **Emotional insight**: Understand how customers feel, not just what they say
- **Training data**: Identify what creates positive vs negative experiences
- **Quality assurance**: Spot problematic calls for review

**Sentiment analysis in action**:

**Positive sentiment examples**:

```
Transcript: "Wow, that was so easy! Thank you so much!"
Sentiment: 0.92 (Positive)
Indicators: "Wow", "easy", "thank you", exclamation marks

Transcript: "Perfect, that's exactly what I needed."
Sentiment: 0.78 (Positive)
Indicators: "Perfect", "exactly", satisfaction with outcome
```

**Neutral sentiment examples**:

```
Transcript: "Okay, what's my account balance?"
Sentiment: 0.50 (Neutral)
Indicators: Straightforward question, no emotion

Transcript: "I need to update my address."
Sentiment: 0.52 (Neutral)
Indicators: Matter-of-fact request, transactional
```

**Negative sentiment examples**:

```
Transcript: "This is ridiculous. I've been trying for 10 minutes!"
Sentiment: 0.15 (Negative)
Indicators: "ridiculous", frustration, complaints

Transcript: "No, that's not what I asked. Listen to me."
Sentiment: 0.22 (Negative)
Indicators: "No", dismissive, demanding tone
```

<Warning>
Sentiment analysis isn't perfect. Sarcasm, cultural differences, and context can cause misclassification. Use sentiment as a guide, not gospelâ€”review actual transcripts for confirmation.
</Warning>

**Using sentiment data**:

```
Daily monitoring:
- Check if negative sentiment is increasing
- Review a few negative calls to understand why
- Quick action if sudden spike

Weekly analysis:
- Compare sentiment distribution to last week
- Identify any trending issues
- Review highest negative sentiment calls

Monthly deep dive:
- Correlate sentiment with other metrics
  * Negative sentiment + High FCR = Bad resolutions?
  * Positive sentiment + Long duration = Thorough service?
- Extract lessons from most positive calls
- Create playbook from what works
```

**Sentiment trajectory patterns**:

```
Pattern 1: Starts negative, ends positive
Initial: "I have a problem..." (Negative 0.3)
Middle: AI helps...
Final: "Great, thank you!" (Positive 0.8)

Meaning: Excellent recovery
- Customer had issue, AI solved it
- This is the ideal pattern

Pattern 2: Starts neutral, ends negative
Initial: "I need help with..." (Neutral 0.5)
Middle: AI struggles to help...
Final: "Never mind, this is useless" (Negative 0.2)

Meaning: Failed to help
- Escalation opportunity missed
- Review these calls to improve training

Pattern 3: Consistently positive
Initial: "Hi there!" (Positive 0.7)
Middle: "That's helpful" (Positive 0.75)
Final: "Thanks so much!" (Positive 0.85)

Meaning: Smooth experience
- Use as training examples
- Understand what made it work

Pattern 4: Consistently negative
Initial: "I'm very frustrated" (Negative 0.25)
Middle: "This isn't helping" (Negative 0.18)
Final: "I give up" (Negative 0.12)

Meaning: Complete failure
- Urgent review needed
- Should have escalated early
```

<CardGroup cols={2}>
  <Card title="Sentiment as Early Warning" icon="bell">
    A spike in negative sentiment often precedes a drop in CSAT. Monitor sentiment daily to catch issues early.
  </Card>
  <Card title="Learn from Positive Sentiment" icon="lightbulb">
    Study your most positive calls. What did the AI do right? Replicate those patterns across all conversations.
  </Card>
</CardGroup>

## Sentiment Distribution Chart

The donut chart visualizes the sentiment breakdown for easy interpretation:

### Chart Features

**Donut segments**:

- **Green**: Positive sentiment
- **Yellow/Gray**: Neutral sentiment
- **Red**: Negative sentiment

**Segment size**: Proportional to percentage

- Larger segment = more calls with that sentiment

**Hover interaction**:

- Shows exact percentage
- Displays count of calls

**Center text**: May display total calls or dominant sentiment

### How to Read the Chart

**Quick visual assessment**:

```
Healthy chart:
- Green (positive) is largest segment
- Red (negative) is smallest segment
- Looks like: ðŸŸ¢ðŸŸ¢ðŸŸ¢âšªâšªðŸ”´

Warning chart:
- Red (negative) is large
- Green (positive) is small
- Looks like: ðŸŸ¢âšªâšªðŸ”´ðŸ”´ðŸ”´

Neutral chart:
- Yellow/gray (neutral) dominates
- May indicate transactional, not engaging
- Looks like: ðŸŸ¢âšªâšªâšªâšªðŸ”´
```

**Interpreting patterns**:

```
Scenario 1: 70% Positive, 20% Neutral, 10% Negative
Visual: Large green, medium yellow, small red
Interpretation: Healthy, customers mostly satisfied
Action: Maintain current approach

Scenario 2: 40% Positive, 35% Neutral, 25% Negative
Visual: Moderate green, large yellow, large red
Interpretation: Too many unhappy customers
Action: Urgent investigation needed

Scenario 3: 30% Positive, 60% Neutral, 10% Negative
Visual: Small green, huge yellow, small red
Interpretation: Functional but not delightful
Action: Look for ways to create positive experiences
```

<Tip>
Screenshot the sentiment chart weekly and create a timeline. Visual trends (chart changing from mostly green to more red) are powerful for communicating issues to stakeholders.
</Tip>

## Filtering Satisfaction Data

Apply filters to analyze specific segments:

### Agent Comparison

Compare satisfaction across different voice agents:

**Example: Benchmarking agents**

```
Agent A Analysis:
- CSAT: 82%
- NPS: 45
- FCR: 78%
- Escalation: 18%
- Sentiment: 68% Positive, 22% Neutral, 10% Negative

Agent B Analysis:
- CSAT: 68%
- NPS: 15
- FCR: 62%
- Escalation: 32%
- Sentiment: 48% Positive, 30% Neutral, 22% Negative

Conclusion:
Agent A significantly outperforms Agent B across all metrics

Next steps:
1. Review Agent A's configuration
2. Identify differences (prompts, tools, training data)
3. Apply learnings to Agent B
4. Re-measure after improvements
```

**Why satisfaction varies by agent**:

- Different conversation flows
- Different personality/tone
- Different use cases or complexity levels
- Different training data quality
- Different integration capabilities

<Note>
When comparing agents, ensure they handle similar types of queries. An agent handling complex technical support will naturally have different metrics than a simple FAQ bot.
</Note>

### Time Period Analysis

Track satisfaction trends over time:

**Example: Identifying declining satisfaction**

```
This Month:
- CSAT: 72%
- NPS: 28
- Negative Sentiment: 18%

Last Month:
- CSAT: 79%
- NPS: 38
- Negative Sentiment: 12%

Two Months Ago:
- CSAT: 81%
- NPS: 42
- Negative Sentiment: 10%

Pattern: Declining satisfaction over time

Investigation:
1. What changed two months ago?
   - New LLM version?
   - Updated prompts?
   - Increased volume/complexity?

2. Review customer feedback for themes
   - "AI doesn't understand me" (mentioned frequently)
   - "Takes too long" (increasing mentions)

3. Check performance metrics
   - Latency increased from 2,200ms to 2,800ms
   - Error rate up from 2% to 4%

Hypothesis: Recent system changes degraded performance,
leading to slower, less reliable service

Action plan:
- Roll back recent changes
- Optimize for speed
- Improve error handling
- Monitor for improvement
```

**Seasonal patterns to watch for**:

```
Holiday season:
- Higher expectations
- More complex queries (gifts, returns)
- CSAT may dip naturally
- Plan: Staff human escalation support

Quarter-end:
- Business customers more urgent
- Less patience for delays
- Higher escalation expected
- Plan: Expedite business queries

Post-product launch:
- Influx of questions about new features
- AI may not have all answers yet
- Expect higher escalations temporarily
- Plan: Rapid agent training on new features
```

## Best Practices

### Setting Satisfaction Targets

**SMART goals for satisfaction metrics**:

```
Example 1: CSAT improvement
- Specific: Increase CSAT from 75% to 82%
- Measurable: Track weekly CSAT scores
- Achievable: 7-point improvement over 3 months
- Relevant: Higher CSAT reduces churn
- Time-bound: Achieve by end of Q2

Actions:
- Review low-CSAT calls for common issues
- Implement top 3 fixes
- Train agent on better responses
- Monitor weekly progress

Example 2: NPS growth
- Specific: Improve NPS from 25 to 40
- Measurable: Monthly NPS surveys
- Achievable: 15-point increase over 6 months
- Relevant: Higher NPS drives referrals
- Time-bound: Achieve by year-end

Actions:
- Identify what promoters love
- Address top detractor complaints
- Turn passives into promoters
- Track monthly trend

Example 3: Reduce escalations
- Specific: Lower escalation rate from 25% to 18%
- Measurable: Daily escalation tracking
- Achievable: 7-point reduction over 2 months
- Relevant: Improves AI ROI
- Time-bound: Complete by end of quarter

Actions:
- Analyze top escalation reasons
- Expand AI capabilities for top 3 reasons
- Improve escalation experience when needed
- Monitor daily rate
```

<CardGroup cols={2}>
  <Card title="Start Where You Are" icon="map-marker">
    Don't compare to industry benchmarks initially. Measure your baseline, then aim for 10-20% improvement.
  </Card>
  <Card title="Celebrate Wins" icon="trophy">
    When you hit a target, acknowledge the achievement and set a new stretch goal.
  </Card>
</CardGroup>

### Closing the Feedback Loop

**How to act on satisfaction data**:

```
Step 1: Collect feedback
- CSAT/NPS surveys after calls
- Sentiment analysis from transcripts
- Direct customer comments
- Support ticket themes

Step 2: Analyze patterns
- Group feedback by theme
- Rank by frequency and impact
- Identify root causes
- Prioritize fixes

Step 3: Implement improvements
- Agent training updates
- System configuration changes
- Process improvements
- Bug fixes

Step 4: Measure impact
- Did CSAT improve?
- Did NPS increase?
- Did sentiment shift positive?
- Are detractors decreasing?

Step 5: Repeat
- Continuous improvement cycle
- Always working on next issue
- Never "done" optimizing
```

**Example closed-loop improvement**:

```
Week 1: Identify issue
- Negative sentiment spike
- Common complaint: "AI repeats itself"
- 15% of calls mention this

Week 2: Root cause
- Agent configuration had redundant prompts
- Causing AI to say same thing twice
- Easy fix

Week 3: Implement fix
- Update prompt templates
- Remove redundancies
- Test with sample calls

Week 4: Measure impact
- Negative sentiment down from 18% to 12%
- CSAT up from 74% to 78%
- "Repeats itself" complaints down to 3%
- Success! Document for future reference
```

<Tip>
Create a "feedback log" tracking what you heard, what you did, and what happened. Over time, this becomes invaluable institutional knowledge about what works.
</Tip>

### Proactive Satisfaction Monitoring

**Daily quick checks (2-3 minutes)**:

```
Morning routine:
1. Check yesterday's CSAT
   - Compare to weekly average
   - Any significant change?

2. Review sentiment distribution
   - Negative sentiment spike?
   - Check a few negative calls

3. Scan NPS responses
   - Any new detractor comments?
   - Note themes

If issues found:
- Flag for deeper investigation
- Alert team if urgent
- Schedule time to analyze
```

**Weekly deep dives (30-45 minutes)**:

```
Weekly satisfaction review:
1. Calculate week-over-week changes
   - CSAT trend
   - NPS trend
   - Escalation trend
   - Sentiment distribution shift

2. Read detractor comments
   - What made them unhappy?
   - Any common threads?
   - Actionable feedback?

3. Read promoter comments
   - What delighted them?
   - Can we replicate this?
   - Use in marketing?

4. Review escalated calls
   - Why did they escalate?
   - Could AI have handled it?
   - Training opportunity?

5. Document findings
   - Top 3 issues this week
   - Top 3 wins this week
   - Actions for next week
```

**Monthly strategic reviews (1-2 hours)**:

```
Monthly satisfaction strategy:
1. Month-over-month comparison
   - All key metrics
   - Trend direction
   - Goal progress

2. Identify systematic issues
   - Recurring problems
   - Structural limitations
   - Training gaps

3. Plan improvements
   - Next month's focus area
   - Resource allocation
   - Success criteria

4. Stakeholder reporting
   - Create executive summary
   - Highlight wins and challenges
   - Propose investments if needed

5. Set next month's goals
   - Based on current performance
   - Aligned with business objectives
   - Realistic but ambitious
```

## Troubleshooting

### Low Response Rates on Surveys

**Problem**: CSAT/NPS data but very few survey responses

**Solutions**:

```
Issue 1: Survey too long
- Solution: Simplify to 1-2 questions max
- Example: Just ask CSAT, make NPS optional

Issue 2: Survey timing wrong
- Solution: Ask immediately after call, not hours later
- Strike while experience is fresh

Issue 3: Survey friction too high
- Solution: Make it one-click when possible
- DTMF: "Press 1-5 to rate your experience"
- SMS: "Reply 1-5 to rate us"

Issue 4: No incentive to respond
- Solution: Small reward for feedback
- "We'll donate $1 to charity for each response"
- Entry into prize drawing

Issue 5: Fatigue from over-surveying
- Solution: Don't ask on every call
- Sample 20-30% of calls randomly
```

<Note>
A 20-30% survey response rate is considered good for phone surveys. If you're getting less than 10%, your survey design likely needs improvement.
</Note>

### Sentiment Doesn't Match CSAT

**Problem**: Positive sentiment but low CSAT, or vice versa

**Possible explanations**:

```
Scenario 1: Positive sentiment, low CSAT
Possible causes:
- AI is friendly but doesn't solve problem
- Customers are polite but unsatisfied
- Sentiment analysis missing sarcasm

Investigation:
- Review transcripts manually
- Look for patterns like:
  "Thanks for trying" (polite but unresolved)
  "I guess that helps" (lukewarm, not truly satisfied)

Fix:
- Focus on outcomes, not just politeness
- Ensure FCR is high
- Don't mistake nicety for satisfaction

Scenario 2: Negative sentiment, high CSAT
Possible causes:
- Customers start frustrated but get helped
- Sentiment measures start, CSAT measures end
- Customer vents but appreciates resolution

Investigation:
- Check sentiment trajectory (does it improve?)
- Review if AI handles frustrated customers well

Fix:
- This might be okay if you're recovering well
- Document what turns situations around
- Train on de-escalation techniques
```

<Warning>
Sentiment analysis measures emotion; CSAT measures satisfaction. They're related but not the same. A customer can be emotionally neutral but highly satisfied (problem solved efficiently) or emotionally positive but unsatisfied (nice AI that didn't help).
</Warning>

### Satisfaction Metrics Not Loading

**Problem**: Tab shows loading state or "No Data Available"

**Troubleshooting steps**:

```
Check 1: Survey data exists
- Have customers been surveyed?
- Is survey collection working?
- Try expanding date range

Check 2: Sentiment analysis enabled
- Some systems require explicit enablement
- Check with admin if sentiment should be available

Check 3: Filters too restrictive
- Try "All agents"
- Expand date range
- See if data appears

Check 4: Browser/network
- Check browser console for errors
- Verify network connection
- Try refreshing page

Check 5: Permissions
- Ensure you have access to satisfaction data
- Some orgs restrict this to managers

If still not working:
- Contact system administrator
- May be backend issue or configuration problem
```

### Conflicting Metrics

**Problem**: One metric looks good, another looks bad

**Example scenarios**:

```
Scenario: High FCR, Low CSAT
Conflict: Supposedly resolving calls, but customers unhappy

Possible causes:
- Marking calls "resolved" incorrectly
- AI provides wrong answers confidently
- Customers don't realize they got wrong info

Investigation:
- Audit "resolved" calls for accuracy
- Check if customers calling back later
- Review transcripts for quality

Scenario: Low Escalation, High Negative Sentiment
Conflict: AI isn't escalating, but customers frustrated

Possible causes:
- AI avoiding escalation when it should
- Customers don't know escalation option exists
- AI arguing with frustrated customers

Investigation:
- Review negative sentiment calls
- Check if escalation was needed but didn't happen
- Improve escalation triggers

Scenario: High NPS, Low FCR
Conflict: Customers would recommend, but not resolving

Possible causes:
- Customers love the experience even if unresolved
- NPS captures brand sentiment, not call outcome
- Perhaps human escalations are excellent

Investigation:
- This might be okay if humans handle well
- But should still work to improve AI resolution
```

<Tip>
When metrics conflict, there's always a story behind it. Don't ignore the conflictâ€”dig in to understand what's really happening. The insights are often the most valuable.
</Tip>

## Advanced Analysis

### Correlation Analysis

**Connecting satisfaction to other metrics**:

```
Question: Does latency affect satisfaction?

Method:
1. Group calls by latency:
   - Fast (< 2s): CSAT = 85%
   - Medium (2-3s): CSAT = 78%
   - Slow (> 3s): CSAT = 68%

Finding: Strong correlation
Action: Prioritize performance optimization

Question: Does call duration affect satisfaction?

Method:
1. Group by duration:
   - Very short (< 30s): CSAT = 60%, FCR = 40%
   - Short (30-120s): CSAT = 82%, FCR = 85%
   - Long (> 120s): CSAT = 70%, FCR = 75%

Finding: Sweet spot is 30-120s
Action: Neither too quick nor too long is ideal
```

### Cohort Analysis

**Tracking satisfaction across different customer groups**:

```
By Channel:
- Phone: CSAT 80%, NPS 38
- Web: CSAT 75%, NPS 32
- WhatsApp: CSAT 82%, NPS 45

Insight: WhatsApp users happiest
Hypothesis: Self-selected tech-savvy users
Action: Learn from WhatsApp experience

By Time of Day:
- Morning (9-12): CSAT 78%
- Afternoon (12-17): CSAT 82%
- Evening (17-21): CSAT 75%

Insight: Afternoon best, evening worst
Hypothesis: System load? User fatigue?
Action: Investigate evening calls

By Agent Type:
- Agent A (FAQ): CSAT 88%, Escalation 8%
- Agent B (Support): CSAT 75%, Escalation 22%
- Agent C (Sales): CSAT 71%, Escalation 28%

Insight: Simpler use case = higher satisfaction
Expected: More complex = lower scores
Action: Set appropriate benchmarks by agent type
```

## Related Documentation

<CardGroup cols={2}>
  <Card title="Dashboard Overview" icon="chart-line" href="/dashboard/overview">
    View overall system activity and session metrics
  </Card>
  <Card title="Performance Metrics" icon="gauge" href="/dashboard/performance">
    Analyze technical performance and response times
  </Card>
  <Card title="Live Calls" icon="phone" href="/dashboard/live-calls">
    Monitor active calls and listen to conversations
  </Card>
  <Card title="Call History" icon="history" href="/dashboard/call-history">
    Review individual call transcripts and outcomes
  </Card>
</CardGroup>

## Summary

Customer satisfaction is the ultimate measure of your voice AI's success. Use the Satisfaction & Outcome tab to:

- **Measure** satisfaction with CSAT, NPS, and sentiment analysis
- **Monitor** resolution effectiveness with FCR and escalation rates
- **Identify** issues early through sentiment trends and negative feedback
- **Improve** continuously by closing the feedback loop
- **Prove ROI** by demonstrating customer advocacy and reduced escalations

Remember: metrics are insights, not just numbers. Behind every percentage is a real customer experience. Use this data to understand and serve your customers better.
